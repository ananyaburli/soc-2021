{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "obtKK2Ga7VPN"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "iris=datasets.load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZDR0_zh2h7U"
      },
      "source": [
        "X = iris.data\n",
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXH30sKL4kUD"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.5, random_state = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87tMmX5OoHDs"
      },
      "source": [
        "1.       \n",
        " Using Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrCu0CjK7-r3",
        "outputId": "86c257e0-e780-47b2-d32e-c2cc12f6b03c"
      },
      "source": [
        "logistic_regression = LogisticRegression()\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "LogisticRegression(C=1.0, class_weight = None, dual = False, fit_intercept = True, intercept_scaling = 1, l1_ratio = None, max_iter = 100, multi_class = 'ovr', n_jobs = None, penalty = 'l2', random_state = 3, solver = 'sag', tol = 0.0001, verbose = 0, warm_start = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='ovr', n_jobs=None, penalty='l2', random_state=3,\n",
              "                   solver='sag', tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7hhrV1zD9oA",
        "outputId": "8dd8c2e7-9566-4b20-d198-b1c56950ce46"
      },
      "source": [
        "y_pred = logistic_regression.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "accuracy_percentage = 100*accuracy\n",
        "print(\"accuracy =\", accuracy_percentage, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 96.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_u-jY24FCPS"
      },
      "source": [
        "2. Using Gaussian Naive Bayes algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzOAYnzmEbgK",
        "outputId": "e86277b9-459e-4a63-cc3b-d5e806145322"
      },
      "source": [
        "# training the model on the dataset\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB6y1WYtFw5F",
        "outputId": "880512b2-01b2-4f6a-bb33-d4bec91054ce"
      },
      "source": [
        "#making predictions on the testing set\n",
        "y_pred2 = gnb.predict(X_test)\n",
        "accuracy2 = metrics.accuracy_score(y_test, y_pred2)\n",
        "accuracy_percentage2 = 100*accuracy2\n",
        "print(\"Gaussian Naive Bayes Model accuracy =\", accuracy_percentage2, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gaussian Naive Bayes Model accuracy = 96.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnYzti_8JtIA"
      },
      "source": [
        "Accuracy obtained by using logistic regression model and Gaussian Naive Bayes Model is the same since there was no gaussian function in this case, accuracy results will be the same. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUa208-seCsV"
      },
      "source": [
        "3.\n",
        "\n",
        "**Overfitting** is a situation when a model shows almost perfect accuracy when handling training data. This situation happens when the model has a complex set of rules. When a model is overfitting, it can be inaccurate when handling new data.\n",
        "\n",
        "**Underfitting** is when a model doesn't fit the training data due to sets of rules that are too simple. You can't rely on an underfitting model to make an accurate prediction.\n",
        "\n",
        "When we increase number of features, we are essentially making the sets of rules complex.The accuracy for predicting for a new data set will thus, increase upto a number of random_sets after which the predictions become increasingly inaccurate.\n",
        "\n",
        "Accuracy is max when the complexity of sets of rules is optimum.Let's call the number of features in this case n . \n",
        "\n",
        "For no. of features < n, we tend towards an underfitting model.\n",
        "\n",
        "For no. of features > n, we tend towards an overfittting model."
      ]
    }
  ]
}